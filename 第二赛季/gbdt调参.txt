boosting主要目标是降低偏差。

1、子模型数和学习率需要联合调整才能尽可能地提高模型的准确度

---（子模型越复杂，偏差越低，方差越高，粗粒度调整树的结构）
2、最大叶节点数
3、最大树深度

4、分裂所需最小样本数
5、子采样率
6、叶节点最小样本数
7、最大特征数
8、叶节点最小权重总值

数加平台GBDT参数
1、metric类型
2、树的数目
3、学习速率
4、训练集采集样本比例
5、训练集采集特征比例

7、测试数据比例


6、最大叶子数
8、树最大深度
9、叶节点最少样本数

10、随机数产生器种子
11、一个特征分裂的最大数量

方法一：
step1.调整过程影响类参数
“子模型数”和“学习率”：先选取一个一般的、最后再调整
step2.调整子模型影响类参数
1.“分裂条件”
2.“分裂时参与判断的最大特征数”
3.“最大深度”（会抖动）
4.“分裂所需的最小样本数”
5.“叶节点最小样本数”
6.“最大叶节点数”（会抖动）

方法二：
1.选择一个相对高的learning_rate。缺省值为0.1，通常在0.05到0.2之间都应有效
2.根据这个learning_rate，去优化树的数目。这个范围在[40,70]之间。记住，选择可以让你的电脑计算相对较快的值。因为结合多种情况才能决定树的参数.
3.调整树参数，来决定learning_rate和树的数目。注意，我们可以选择不同的参数来定义树。
4.调低learning_rate，增加estimator的数目，来得到更健壮的模型

3里面基于tree的参数调节，有顺序要求
3.1.调节max_depth 和 num_samples_split
3.2.调节min_samples_leaf
33.调节max_features

然后调节subsample

、树的数目
、学习速率 
、、最大叶子数
、、树最大深度（不调吧，和最大叶子数是重复的）
、、叶节点最少样本数
、、、训练集采集样本比例
、、、训练集采集特征比例
、树的数目
、学习速率 